# Data Science and Decision Making

### Overall

This is a new module on Data Science and Decision Making - we will examine most aspects of modern data science and try to create a fully-fledged end-to-end data science study. The module outline can be found [here](https://www.essex.ac.uk/modules/Default.aspx?coursecode=CE888&year=17) - but it is still subject to changes. The lecture notes and lab scripts for this course will be made available and further developed during the module.  

This is a very hands-on module, where the goal is to give you sufficient breadth and depth to work as an independent data scientist. The course is assessed solely through coursework. 



### Lectures
Lectures take place on **Monday 14:00-16:00 at Room NTC.2.04**. 

<a id="lec1"></a> 

1. [Lecture 1: Introduction](./slides/01-Introduction-slides.pdf), [Handouts](./slides/01-Introduction-handouts.pdf) 


<a id="lec2"></a> 

2. [Lecture 2: Summary and resampling statistics](./slides/02-Stats-Slides.pdf), [Handouts](./slides/02-Stats-handouts.pdf) 


<a id="lec3"></a> 

3. [Lecture 3: Predictive modelling](./slides/03-Modelling-slides.pdf), [Handouts](./slides/03-Modelling-handouts.pdf) 

<a id="lec4"></a> 

4. [Lecture 4: Recommender systems](./slides/04-Recommender-slides.pdf), [Handouts](./slides/04-Recommender-handouts.pdf) 

<a id="lec5"></a> 

5. [Lecture 5: Bandits](./slides/05-Bandits-slides.pdf), [Handouts](./slides/05-Bandits-handouts.pdf) 

<a id="lec6"></a> 

6. [Lecture 6: Data exploration](./slides/06-Exploration-slides.pdf), [Handouts](./slides/06-Exploration-handouts.pdf) 

<!--

<a id="lec7"></a> 

7. [Lecture 7: Neural networks](./slides/07-Neural-slides.pdf), [Handouts](./slides/07-Neural-handouts.pdf) 

<a id="lec8"></a> 

8. [Lecture 8: Images, Video, Audio, Text](./slides/08-Text-slides.pdf), [Handouts](./slides/08-Text-handouts.pdf) 

<a id="lec9"></a> 

9. [Lecture 9: Data and Systems](./slides/09-Systems-slides.pdf), [Handouts](./slides/09-Systems-handouts.pdf) 

<a id="lec10"></a> 

10. [Lecture 10: Discussion](./slides/10-Discussion-slides.pdf), [Handouts](./slides/10-Discussion-handouts.pdf) 

-->



### Labs
Labs are every **Tuesday 12:00-15:00, CES Lab 6,7**. Labs are assessed and they should be completed either in class or later on. 



Please download the lab Virtual Machine from here: [MLVM2 Virtual Machine](https://drive.google.com/drive/folders/1SwdnpeJfjzUH7YDgwgrtDb7mve6d4zIe)


All labs can be found here: [https://github.com/ssamot/ce888/tree/master/labs/](https://github.com/ssamot/ce888/tree/master/labs/)

<a id="lab1"></a>

1. [Lab 1: VM setup and simple emotion detection](https://github.com/ssamot/ce888/tree/master/labs/lab1) 


<a id="lab2"></a>

2. [Lab 2: Creating plots, overleaf and confidence bounds](https://github.com/ssamot/ce888/tree/master/labs/lab2) 


<a id="lab3"></a>

3. [Lab 3: Jupiter/IPython and Classification](https://github.com/ssamot/ce888/tree/master/labs/lab3) 

<a id="lab4"></a>

4. [Lab 4 and 5: Recommender systems and jokes](https://github.com/ssamot/ce888/tree/master/labs/lab4_new) 





<!-- <a id="lab6"></a>

4. [Lab 4: Bandits and time series plotting](https://github.com/ssamot/ce888/tree/master/labs/lab4) 

 -->

<a id="lab6"></a>






6. [Lab 6: Clustering and visualisation](https://github.com/ssamot/ce888/tree/master/labs/lab6) 

<!--

<a id="lab7"></a>

7. [Lab 7: Neural networks and MNIST](https://github.com/ssamot/ce888/tree/master/labs/lab7) 

<a id="lab8"></a>

8. [Lab 8: IMDB and text data](https://github.com/ssamot/ce888/tree/master/labs/lab8) 
-->


### IDEs
You can use whatever IDE you want for the course, however my proposal would be to use PyCharm - which is installed in the lab machines:

*  [PyCharm](https://www.jetbrains.com/pycharm/)


### Assessment

The objective of the module and the main assignment is to produce a Data Science app (i.e. make use of data to generate results, make inferences, present the results to third parties) and write down the description of the methods and the results in a scientific paper. Some ideas for possible apps are provided [here](#assignment-suggestions).

* Assigments
	* [Assignment 1](./assignments/ce888-assignment-1.pdf)

<!--
	* [Assignment 2](./assignments/ce888-assignment-2.pdf)
-->

	

See FASER for exact assignment deadlines. 


### Readings

Every lecture will come with a set of online reading suggestions - they will be added here. 

**Lecture 1**

[James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.](https://s3.amazonaws.com/academia.edu.documents/37162300/An_Introduction_to_Statistical_Learning_with_Applications_in_R.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1515959849&Signature=1vCfZn0dpIe%2FL45pgI2fjn9GrlI%3D&response-content-disposition=inline%3B%20filename%3DPrinter_Opaque_this_An_Introduction_to_S.pdf)

[Breiman, Leo. "Statistical modeling: The two cultures (with comments and a rejoinder by the author)." Statistical Science 16.3 (2001): 199-231.](http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726%20)

[Anderson, Philip W. "More is different." Science 177.4047 (1972): 393-396.](https://www.tkm.kit.edu/downloads/TKM1_2011_more_is_different_PWA.pdf)


**Lecture 2**

[Efron, Bradley, and Robert J. Tibshirani. An introduction to the bootstrap. CRC press, 1994.](http://cds.cern.ch/record/526679/files/0412042312_TOC.pdf)

[Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. "False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant." Psychological science 22.11 (2011): 1359-1366.](http://www.haas.berkeley.edu/groups/online_marketing/facultyCV/papers/nelson_false-positive.pdf)

[Schenker, Nathaniel, and Jane F. Gentleman. "On judging the significance of differences by examining the overlap between confidence intervals." The American Statistician 55.3 (2001): 182-186.](https://www.jstor.org/stable/2685796)


**Lecture 3** 

[Jake VanderPlas, Python Data Science Handbook Essential Tools for Working with Data 2016. O'Reilly Media, 2016](https://github.com/jakevdp/PythonDataScienceHandbook)

[Kohavi, Ron. "A study of cross-validation and bootstrap for accuracy estimation and model selection." IJCAI. Vol. 14. No. 2. 1995.](https://pdfs.semanticscholar.org/0be0/d781305750b37acb35fa187febd8db67bfcc.pdf)

[Geman, Stuart, Elie Bienenstock, and René Doursat. "Neural networks and the bias/variance dilemma." Neural computation 4.1 (1992): 1-58.](https://stuff.mit.edu/afs/athena.mit.edu/course/6/6.435/www/Geman92.pdf)

[scikit-learn's website](http://scikit-learn.org/)

**Lecture 4**

[Simon Funk, Netflix Update: Try This at Home](http://sifter.org/~simon/journal/20061211.html)

[Barkan, Oren, and Noam Koenigstein. "Item2vec: neural item embedding for collaborative filtering." Machine Learning for Signal Processing (MLSP), 2016 IEEE 26th International Workshop on. IEEE, 2016.](https://arxiv.org/pdf/1603.04259.pdf)

[Hu, Yifan, Yehuda Koren, and Chris Volinsky. "Collaborative filtering for implicit feedback datasets." Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on. Ieee, 2008.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.5120&rep=rep1&type=pdf)




**Lecture 4**

[White, John. Bandit algorithms for website optimization. " O'Reilly Media, Inc.", 2012.](http://shop.oreilly.com/product/0636920027393.do)

[Oza, Nikunj C. "Online bagging and boosting." Systems, man and cybernetics, 2005 IEEE international conference on. Vol. 3. IEEE, 2005.](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20050239012.pdf)

[Osband, Ian, et al. "Deep exploration via bootstrapped DQN." Advances In Neural Information Processing Systems. 2016.](http://papers.nips.cc/paper/6500-deep-exploration-via-bootstrapped-dqn.pdf)



**Lecture 6**

[Rousseeuw, Peter J. "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis." Journal of computational and applied mathematics 20 (1987): 53-65.](http://www.sciencedirect.com/science/article/pii/0377042787901257)

[Arthur, David, and Sergei Vassilvitskii. "k-means++: The advantages of careful seeding." Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2007.](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)

<!--

**Lecture 7**

[Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. "Reducing the dimensionality of data with neural networks." science 313.5786 (2006): 504-507.](https://pdfs.semanticscholar.org/7d76/b71b700846901ac4ac119403aa737a285e36.pdf)

[Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT Press, 2016.](http://www.deeplearningbook.org/)

[Bengio, Yoshua. "Learning deep architectures for AI." Foundations and trends® in Machine Learning 2.1 (2009): 1-127.](http://www.nowpublishers.com/article/DownloadSummary/MAL-006)


**Lecture 8**

[Halevy, Alon, Peter Norvig, and Fernando Pereira. "The unreasonable effectiveness of data." IEEE Intelligent Systems 24.2 (2009): 8-12.](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf)

[Zhou, Zhi-Hua, and Ji Feng. "Deep Forest: Towards An Alternative to Deep Neural Networks." arXiv preprint arXiv:1702.08835 (2017).](https://arxiv.org/pdf/1702.08835.pdf)

**Lecture 9** 

[Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing on large clusters." Communications of the ACM 51.1 (2008): 107-113.](https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/dean/dean_html/)
[Karau, Holden, et al. Learning spark: lightning-fast big data analysis. " O'Reilly Media, Inc.", 2015.](http://shop.oreilly.com/product/0636920028512.do)

-->

### People
* Module Supervisor: *Spyros Samothrakis*, <ssamot@essex.ac.uk>

* * * 
* * * 





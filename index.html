<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>CE888 2018</title>

    <!-- Bootstrap -->
    <link href="./bootstrap/css/bootstrap.cr.css" rel="stylesheet">
    <link href="./ce888.css" rel="stylesheet">
    <link rel="shortcut icon" href="graphics/favicon.ico">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>
  <body>
<nav class="navbar navbar-default navbar-fixed-top">

     
      <div class="container">
       <a href="#data-science-and-decision-making" class="navbar-brand">CE888</a>
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
           
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">


            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#" id="Lectures">Lectures <span class="caret"></span></a>
              <ul class="dropdown-menu" aria-labelledby="Lectures">
                
                <li class="divider"></li>
                <li><a href="#lec1">Lecture 1</a></li>
                <li><a href="#lec2">Lecture 2</a></li>
                <li><a href="#lec3">Lecture 3</a></li>
                <li><a href="#lec4">Lecture 4</a></li>
                <li><a href="#lec5">Lecture 5</a></li>
                <li><a href="#lec6">Lecture 6</a></li>
                <li><a href="#lec7">Lecture 7</a></li>
                <li><a href="#lec8">Lecture 8</a></li>
                <li><a href="#lec9">Lecture 9</a></li>
                <li><a href="#lec10">Lecture 10</a></li>
    
              </ul>
            </li>

            <li class="dropdown">
              <a class="dropdown-toggle" data-toggle="dropdown" href="#" id="Labs">Labs <span class="caret"></span></a>
              <ul class="dropdown-menu" aria-labelledby="Labs">
                
                <li class="divider"></li>
                <li><a href="#lab1">Lab 1</a></li>
                <li><a href="#lab2">Lab 2</a></li>
                <li><a href="#lab3">Lab 3</a></li>
                <li><a href="#lab4">Lab 4</a></li>
                <li><a href="#lab5">Lab 5</a></li>
                <li><a href="#lab6">Lab 6</a></li>
                <li><a href="#lab7">Lab 7</a></li>
                <li><a href="#lab8">Lab 8</a></li>
                <li><a href="#lab9">Lab 9</a></li>
                <li><a href="#lab10">Lab 10</a></li>
    
              </ul>
            </li>

            <li><a href="#assignment-suggestions">Assignments</a></li>

            
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container">
    <div class="row">
    
     
      <div class="span9">
            <h1 id="data-science-and-decision-making">Data Science and Decision Making</h1>
<h3 id="overall">Overall</h3>
<p>This is a new module on Data Science and Decision Making - we will examine most aspects of modern data science and try to create a fully-fledged end-to-end data science study. The module outline can be found <a href="https://www.essex.ac.uk/modules/Default.aspx?coursecode=CE888&amp;year=17">here</a> - but it is still subject to changes. The lecture notes and lab scripts for this course will be made available and further developed during the module.</p>
<p>This is a very hands-on module, where the goal is to give you sufficient breadth and depth to work as an independent data scientist. The course is assessed solely through coursework.</p>
<h3 id="lectures">Lectures</h3>
<p>Lectures take place on <strong>Monday 14:00-16:00 at Room NTC.2.04</strong>.</p>
<p><a id="lec1"></a></p>
<ol style="list-style-type: decimal">
<li><a href="./slides/01-Introduction-slides.pdf">Lecture 1: Introduction</a>, <a href="./slides/01-Introduction-handouts.pdf">Handouts</a></li>
</ol>
<p><a id="lec2"></a></p>
<ol start="2" style="list-style-type: decimal">
<li><a href="./slides/02-Stats-Slides.pdf">Lecture 2: Summary and resampling statistics</a>, <a href="./slides/02-Stats-handouts.pdf">Handouts</a></li>
</ol>
<p><a id="lec3"></a></p>
<ol start="3" style="list-style-type: decimal">
<li><a href="./slides/03-Modelling-slides.pdf">Lecture 3: Predictive modelling</a>, <a href="./slides/03-Modelling-handouts.pdf">Handouts</a></li>
</ol>
<p><a id="lec4"></a></p>
<ol start="4" style="list-style-type: decimal">
<li><a href="./slides/04-Recommender-slides.pdf">Lecture 4: Recommender systems</a>, <a href="./slides/04-Recommender-handouts.pdf">Handouts</a></li>
</ol>
<p><a id="lec5"></a></p>
<ol start="5" style="list-style-type: decimal">
<li><a href="./slides/05-Bandits-slides.pdf">Lecture 5: Bandits</a>, <a href="./slides/05-Bandits-handouts.pdf">Handouts</a></li>
</ol>
<p><a id="lec6"></a></p>
<ol start="6" style="list-style-type: decimal">
<li><a href="./slides/06-Exploration-slides.pdf">Lecture 6: Data exploration</a>, <a href="./slides/06-Exploration-handouts.pdf">Handouts</a></li>
</ol>
<!--

<a id="lec7"></a> 

7. [Lecture 7: Neural networks](./slides/07-Neural-slides.pdf), [Handouts](./slides/07-Neural-handouts.pdf) 

<a id="lec8"></a> 

8. [Lecture 8: Images, Video, Audio, Text](./slides/08-Text-slides.pdf), [Handouts](./slides/08-Text-handouts.pdf) 

<a id="lec9"></a> 

9. [Lecture 9: Data and Systems](./slides/09-Systems-slides.pdf), [Handouts](./slides/09-Systems-handouts.pdf) 

<a id="lec10"></a> 

10. [Lecture 10: Discussion](./slides/10-Discussion-slides.pdf), [Handouts](./slides/10-Discussion-handouts.pdf) 

-->
<h3 id="labs">Labs</h3>
<p>Labs are every <strong>Tuesday 12:00-15:00, CES Lab 6,7</strong>. Labs are assessed and they should be completed either in class or later on.</p>
<p>Please download the lab Virtual Machine from here: <a href="https://drive.google.com/drive/folders/1SwdnpeJfjzUH7YDgwgrtDb7mve6d4zIe">MLVM2 Virtual Machine</a></p>
<p>All labs can be found here: <a href="https://github.com/ssamot/ce888/tree/master/labs/" class="uri">https://github.com/ssamot/ce888/tree/master/labs/</a></p>
<p><a id="lab1"></a></p>
<ol style="list-style-type: decimal">
<li><a href="https://github.com/ssamot/ce888/tree/master/labs/lab1">Lab 1: VM setup and simple emotion detection</a></li>
</ol>
<p><a id="lab2"></a></p>
<ol start="2" style="list-style-type: decimal">
<li><a href="https://github.com/ssamot/ce888/tree/master/labs/lab2">Lab 2: Creating plots, overleaf and confidence bounds</a></li>
</ol>
<p><a id="lab3"></a></p>
<ol start="3" style="list-style-type: decimal">
<li><a href="https://github.com/ssamot/ce888/tree/master/labs/lab3">Lab 3: Jupiter/IPython and Classification</a></li>
</ol>
<p><a id="lab4"></a></p>
<ol start="4" style="list-style-type: decimal">
<li><a href="https://github.com/ssamot/ce888/tree/master/labs/lab4_new">Lab 4 and 5: Recommender systems and jokes</a></li>
</ol>
<!-- <a id="lab6"></a>

4. [Lab 4: Bandits and time series plotting](https://github.com/ssamot/ce888/tree/master/labs/lab4) 

 -->
<p><a id="lab6"></a></p>
<ol start="6" style="list-style-type: decimal">
<li><a href="https://github.com/ssamot/ce888/tree/master/labs/lab6">Lab 6: Clustering and visualisation</a></li>
</ol>
<!--

<a id="lab7"></a>

7. [Lab 7: Neural networks and MNIST](https://github.com/ssamot/ce888/tree/master/labs/lab7) 

<a id="lab8"></a>

8. [Lab 8: IMDB and text data](https://github.com/ssamot/ce888/tree/master/labs/lab8) 
-->
<h3 id="ides">IDEs</h3>
<p>You can use whatever IDE you want for the course, however my proposal would be to use PyCharm - which is installed in the lab machines:</p>
<ul>
<li><a href="https://www.jetbrains.com/pycharm/">PyCharm</a></li>
</ul>
<h3 id="assessment">Assessment</h3>
<p>The objective of the module and the main assignment is to produce a Data Science app (i.e. make use of data to generate results, make inferences, present the results to third parties) and write down the description of the methods and the results in a scientific paper. Some ideas for possible apps are provided <a href="#assignment-suggestions">here</a>.</p>
<ul>
<li>Assigments
<ul>
<li><a href="./assignments/ce888-assignment-1.pdf">Assignment 1</a></li>
</ul></li>
</ul>
<!--
    * [Assignment 2](./assignments/ce888-assignment-2.pdf)
-->
<p>See FASER for exact assignment deadlines.</p>
<h3 id="readings">Readings</h3>
<p>Every lecture will come with a set of online reading suggestions - they will be added here.</p>
<p><strong>Lecture 1</strong></p>
<p><a href="https://s3.amazonaws.com/academia.edu.documents/37162300/An_Introduction_to_Statistical_Learning_with_Applications_in_R.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;Expires=1515959849&amp;Signature=1vCfZn0dpIe%2FL45pgI2fjn9GrlI%3D&amp;response-content-disposition=inline%3B%20filename%3DPrinter_Opaque_this_An_Introduction_to_S.pdf">James, Gareth, et al. An introduction to statistical learning. Vol. 112. New York: springer, 2013.</a></p>
<p><a href="http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726%20">Breiman, Leo. &quot;Statistical modeling: The two cultures (with comments and a rejoinder by the author).&quot; Statistical Science 16.3 (2001): 199-231.</a></p>
<p><a href="https://www.tkm.kit.edu/downloads/TKM1_2011_more_is_different_PWA.pdf">Anderson, Philip W. &quot;More is different.&quot; Science 177.4047 (1972): 393-396.</a></p>
<p><strong>Lecture 2</strong></p>
<p><a href="http://cds.cern.ch/record/526679/files/0412042312_TOC.pdf">Efron, Bradley, and Robert J. Tibshirani. An introduction to the bootstrap. CRC press, 1994.</a></p>
<p><a href="http://www.haas.berkeley.edu/groups/online_marketing/facultyCV/papers/nelson_false-positive.pdf">Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. &quot;False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant.&quot; Psychological science 22.11 (2011): 1359-1366.</a></p>
<p><a href="https://www.jstor.org/stable/2685796">Schenker, Nathaniel, and Jane F. Gentleman. &quot;On judging the significance of differences by examining the overlap between confidence intervals.&quot; The American Statistician 55.3 (2001): 182-186.</a></p>
<p><strong>Lecture 3</strong></p>
<p><a href="https://github.com/jakevdp/PythonDataScienceHandbook">Jake VanderPlas, Python Data Science Handbook Essential Tools for Working with Data 2016. O'Reilly Media, 2016</a></p>
<p><a href="https://pdfs.semanticscholar.org/0be0/d781305750b37acb35fa187febd8db67bfcc.pdf">Kohavi, Ron. &quot;A study of cross-validation and bootstrap for accuracy estimation and model selection.&quot; IJCAI. Vol. 14. No. 2. 1995.</a></p>
<p><a href="https://stuff.mit.edu/afs/athena.mit.edu/course/6/6.435/www/Geman92.pdf">Geman, Stuart, Elie Bienenstock, and René Doursat. &quot;Neural networks and the bias/variance dilemma.&quot; Neural computation 4.1 (1992): 1-58.</a></p>
<p><a href="http://scikit-learn.org/">scikit-learn's website</a></p>
<p><strong>Lecture 4</strong></p>
<p><a href="http://sifter.org/~simon/journal/20061211.html">Simon Funk, Netflix Update: Try This at Home</a></p>
<p><a href="https://arxiv.org/pdf/1603.04259.pdf">Barkan, Oren, and Noam Koenigstein. &quot;Item2vec: neural item embedding for collaborative filtering.&quot; Machine Learning for Signal Processing (MLSP), 2016 IEEE 26th International Workshop on. IEEE, 2016.</a></p>
<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.5120&amp;rep=rep1&amp;type=pdf">Hu, Yifan, Yehuda Koren, and Chris Volinsky. &quot;Collaborative filtering for implicit feedback datasets.&quot; Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on. Ieee, 2008.</a></p>
<p><strong>Lecture 4</strong></p>
<p><a href="http://shop.oreilly.com/product/0636920027393.do">White, John. Bandit algorithms for website optimization. &quot; O'Reilly Media, Inc.&quot;, 2012.</a></p>
<p><a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20050239012.pdf">Oza, Nikunj C. &quot;Online bagging and boosting.&quot; Systems, man and cybernetics, 2005 IEEE international conference on. Vol. 3. IEEE, 2005.</a></p>
<p><a href="http://papers.nips.cc/paper/6500-deep-exploration-via-bootstrapped-dqn.pdf">Osband, Ian, et al. &quot;Deep exploration via bootstrapped DQN.&quot; Advances In Neural Information Processing Systems. 2016.</a></p>
<p><strong>Lecture 6</strong></p>
<p><a href="http://www.sciencedirect.com/science/article/pii/0377042787901257">Rousseeuw, Peter J. &quot;Silhouettes: a graphical aid to the interpretation and validation of cluster analysis.&quot; Journal of computational and applied mathematics 20 (1987): 53-65.</a></p>
<p><a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">Arthur, David, and Sergei Vassilvitskii. &quot;k-means++: The advantages of careful seeding.&quot; Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2007.</a></p>
<!--

**Lecture 7**

[Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. "Reducing the dimensionality of data with neural networks." science 313.5786 (2006): 504-507.](https://pdfs.semanticscholar.org/7d76/b71b700846901ac4ac119403aa737a285e36.pdf)

[Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT Press, 2016.](http://www.deeplearningbook.org/)

[Bengio, Yoshua. "Learning deep architectures for AI." Foundations and trends® in Machine Learning 2.1 (2009): 1-127.](http://www.nowpublishers.com/article/DownloadSummary/MAL-006)


**Lecture 8**

[Halevy, Alon, Peter Norvig, and Fernando Pereira. "The unreasonable effectiveness of data." IEEE Intelligent Systems 24.2 (2009): 8-12.](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf)

[Zhou, Zhi-Hua, and Ji Feng. "Deep Forest: Towards An Alternative to Deep Neural Networks." arXiv preprint arXiv:1702.08835 (2017).](https://arxiv.org/pdf/1702.08835.pdf)

**Lecture 9** 

[Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing on large clusters." Communications of the ACM 51.1 (2008): 107-113.](https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/dean/dean_html/)
[Karau, Holden, et al. Learning spark: lightning-fast big data analysis. " O'Reilly Media, Inc.", 2015.](http://shop.oreilly.com/product/0636920028512.do)

-->
<h3 id="people">People</h3>
<ul>
<li>Module Supervisor: <em>Spyros Samothrakis</em>, <script type="text/javascript">
<!--
h='&#x65;&#x73;&#x73;&#x65;&#120;&#46;&#x61;&#x63;&#46;&#x75;&#x6b;';a='&#64;';n='&#x73;&#x73;&#x61;&#x6d;&#x6f;&#116;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'" clas'+'s="em' + 'ail">'+e+'<\/'+'a'+'>');
// -->
</script><noscript>&#x73;&#x73;&#x61;&#x6d;&#x6f;&#116;&#32;&#x61;&#116;&#32;&#x65;&#x73;&#x73;&#x65;&#120;&#32;&#100;&#x6f;&#116;&#32;&#x61;&#x63;&#32;&#100;&#x6f;&#116;&#32;&#x75;&#x6b;</noscript></li>
</ul>
<hr />
<hr />
<h1 id="assignments">Assignments</h1>
<p>Below you will find a list of projects for CE888 - this is still a draft and I will refine the assignments as we get closer each coursework getting formally released.</p>
<h3 id="rl-and-interpretability">RL and Interpretability</h3>
<p>Modern Reinforcement Learning helps agents learn how to act using complex patterns of text, sound and video and it's slowly moving away from research and making inroads to traditional industries (e.g., creating game NPC characters). The high dimensionality of the input space makes it very hard to interpret why an agent preferred one action over another. In this project we will try to transfer some novel methods from supervised learning to Reinforcement Learning in order to interpret why agents make certain decisions. We will use already existing Atari game playing agents and try to interpret their actions profile in real time, effectively &quot;seeing&quot; through the agent's eyes.</p>
<p><strong>Target Journal/Conference:</strong> IEEE Transactions on Games</p>
<p><strong>References:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/1602.04938v3">Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. &quot;&quot; Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier.&quot; KDD (2016).</a></li>
<li><a href="http://zacklipton.com/media/papers/mythos_model_interpretability_lipton2016.pdf">Lipton, Zachary C., et al. &quot;The Mythos of Model Interpretability.&quot; IEEE Spectrum (2016)</a></li>
<li><a href="https://arxiv.org/pdf/1711.00138">Greydanus, Sam, et al. &quot;Visualizing and Understanding Atari Agents.&quot; arXiv preprint arXiv:1711.00138 (2017).</a></li>
<li><a href="https://docs.opencv.org/3.3.1/d7/d8b/tutorial_py_lucas_kanade.html">OpenCV optical flow tutorial</a></li>
<li><a href="https://github.com/marcotcr/lime">LIME</a></li>
</ol>
<p><strong>Data:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="https://github.com/ppwwyyxx/tensorpack/tree/master/examples/A3C-Gym">Example Open AI gym Atari Controllers - look also in your VM</a></li>
</ol>
<p><strong>Tasks</strong></p>
<ol style="list-style-type: decimal">
<li>Each Atari agent perceives the world through an concatenation of 4 frames and outputs an action. Run agents in at least 6 games, in as diverse states as possible. Collect at least 3,000 observation instances per game and their associated actions and load them for further processing. You will find the code that the agent actually runs here: <a href="https://github.com/ppwwyyxx/tensorpack/blob/master/examples/DeepQNetwork/common.py">common.py</a>. Note that <code>play_one_episode(env, func, render=False)</code> redefines predict using a random search - disable it. You should focus on <code>ob</code> and <code>act</code> variables inside the <code>while True:</code> loop. Also note that some helpful code is:</li>
</ol>
<pre class="{python}"><code>from PIL import Image

stacker = np.empty((84, 0, 3),dtype=&quot;uint8&quot;)

for it in range(4):
    im = Image.fromarray(s[:, :, it*3:3*(it+1)])
    q = np.asarray(im)
    stacker = np.hstack((stacker, q))

im = Image.fromarray(stacker)
im.save(&quot;game_name-&quot; + str(t) + &quot;.png&quot;) # you need to define (t) somewhere so that you know which part of the game you are in. </code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Pass the observations to LIME and get an interpreted image/observation. Save the interpreted images.</li>
<li>Calculate the optical flow between the first image in an observation and the last image.<br />
</li>
<li>Use one of the unsupervised learning algorithms from sci-kit learn to break down your data in various segments - are there clear clusters being formed? What is in each cluster?</li>
<li>Create a video of interpreted agent actions and upload on youtube (optional)</li>
</ol>
<hr />
<h3 id="evolutionary-strategies-for-domain-adaptation">Evolutionary Strategies for Domain Adaptation</h3>
<p>It is often the case that the distribution of the source data is not the same as the target data; for example we only have labeled data examples from images of animals we took in artificial captivity conditions (<em>source data</em>), but we would like to classify animals in the wild (<em>target data</em>). We don't know the labels of the target data, so we have to learn features that fail to discriminate between source and target distributions, but are good enough to actually learn the mapping between those distributions and their labels.</p>
<p><strong>Target Journal/Conference:</strong> IEEE Transactions on Evolutionary Computation</p>
<p><strong>References:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="http://proceedings.mlr.press/v37/ganin15.html">Ganin, Yaroslav, and Victor Lempitsky. &quot;Unsupervised domain adaptation by backpropagation.&quot; International Conference on Machine Learning. 2015.</a></li>
<li><a href="http://zacklipton.com/media/papers/mythos_model_interpretability_lipton2016.pdf">Lipton, Zachary C., et al. &quot;The Mythos of Model Interpretability.&quot; IEEE Spectrum (2016)</a></li>
<li><a href="https://arxiv.org/pdf/1711.00138">Greydanus, Sam, et al. &quot;Visualizing and Understanding Atari Agents.&quot; arXiv preprint arXiv:1711.00138 (2017).</a></li>
<li><a href="https://github.com/ssamot/infoGA">InfoGA archive</a></li>
</ol>
<p><strong>Data:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="https://github.com/pumpikano/tf-dann">MNIST-M Dataset, Blobs and related code</a></li>
<li><a href="https://github.com/jindongwang/transferlearning/blob/master/doc/dataset.md#office-31">The office 31 Dataset</a></li>
</ol>
<p><strong>Tasks</strong></p>
<ol style="list-style-type: decimal">
<li>Download and load the above datasets in python, clearly separating the domain and source data - write down what you observe.</li>
<li>Create a neural network that takes as input the data provided and outputs a set of features - use RELU units.</li>
<li>Use the outputs of the random neural network to train a Random Forest.</li>
<li>Start an evolutionary process using SNES, adapting the weights of the neural network. The score of your classifier should take into account domain adaptation; a good classifier both succeeds in achieving good performance for the source domain, while the features learned fail to discriminate between source and target domains. The score that you give back to SNES should should thus be a weighted sum between how bad a random forest fails to discriminate between source and target, while at the same time how well it does to discriminate between the different classes.</li>
<li>Plot the loss you get at each generation and evaluate your method in both datasets.</li>
</ol>
<hr />
<h3 id="genetic-programmingauto-ml-for-one-shot-learning">Genetic Programming/Auto-ML for One-Shot Learning</h3>
<p>One of the issues of most ML algorithms is the need for copious amounts of data - neural networks are notorious for that. It might be possible to transform our data in a way that algorithms can use a very limited number of examples and still perform well. A possible method for doing this is transforming classification/regression tasks to metric learning tasks, i.e. how far away is a new data instance from ones observed already.</p>
<p><strong>Target Journal/Conference:</strong> IEEE Transactions on Neural Networks and Learning Systems</p>
<p><strong>References:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="https://sorenbouma.github.io/blog/oneshot/">One Shot Learning and Siamese Networks in Keras</a></li>
<li><a href="https://staff.fnwi.uva.nl/t.e.j.mensink/zsl2016/zslpubs/lake15science.pdf">Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. &quot;Human-level concept learning through probabilistic program induction.&quot; Science 350.6266 (2015): 1332-1338.</a></li>
<li><a href="https://github.com/EpistasisLab/tpot">TPOT</a></li>
</ol>
<p><strong>Data:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="https://github.com/brendenlake/omniglot">Omniglot dataset</a></li>
</ol>
<p><strong>Tasks:</strong></p>
<ol style="list-style-type: decimal">
<li>Download and load the omniglot dataset, clearly separating the training and the test data. Generate image combinations (same/different) according the blog post above (reference 2).</li>
<li>Use TPOT to learn a classifier/regressor based on Random Forests that tries to differentiate between objects being in the same or different class.</li>
<li>Use the classifier as a metric function for a scikit-learn <code>sklearn.neighbors.KNeighborsClassifier</code> with a custom metric function that is based on the classifier/regressor learned above.</li>
<li>Revisit the TPOT regressor/classifier by adding <code>class_weight</code> as <code>balanced</code> or <code>balanced_subsample</code> in the parameters of the classifier/regressor.</li>
</ol>
<hr />
<h3 id="continual-learning-using-auto-encoders">Continual Learning using auto-encoders</h3>
<p>One of the most important unsolved issues in Machine Learning is learning concepts incrementally. For this project we will try a novel idea of creating auto-encoders to detect if there has been a change in the setting we find ourselves in.</p>
<p><strong>Target Journal/Conference:</strong> IEEE Transactions on Neural Networks and Learning Systems</p>
<p><strong>References:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="https://blog.keras.io/building-autoencoders-in-keras.html">Keras Autoencoder</a></li>
<li><a href="http://www.pnas.org/content/114/13/3521.full">Kirkpatrick, James, et al. &quot;Overcoming catastrophic forgetting in neural networks.&quot; Proceedings of the National Academy of Sciences (2017): 201611835.</a></li>
<li><a href="https://arxiv.org/pdf/1312.6211.pdf">Goodfellow, Ian J., et al. &quot;An empirical investigation of catastrophic forgetting in gradient-based neural networks.&quot; arXiv preprint arXiv:1312.6211 (2013).</a></li>
</ol>
<p><strong>Data:</strong></p>
<ol style="list-style-type: decimal">
<li><a href="https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py">MNIST dataset</a></li>
<li><a href="https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py">CIFAR10 dataset</a></li>
</ol>
<p><strong>Tasks</strong></p>
<ol style="list-style-type: decimal">
<li>Create three or more tasks of MNIST digits by permuting the pixels (i.e shuffling the images) in a fixed way for each task and save the new datasets in a file. Do the same with CIFAR10 data.</li>
<li>Instantiate <span class="math inline"><em>n</em></span> of keras/neural network autoencoders and associate an instance of a classifier with each one.</li>
<li>Send data in batches and pick the auto-encoder with the lowest error - train the autoencoder and the associated classifier with that batch.</li>
<li>Evaluate the setup in all tasks.</li>
<li>Redo the above experiment with an increased amount of autoencoders (<span class="math inline"><em>n</em> + 1</span>).</li>
<li>Create new autoencoders on the fly if all the the already instantiated autoencoders errors are too high.</li>
</ol>
<hr />
<hr />
<hr />
            </div>
    </div>
  </div>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="bootstrap/js/bootstrap.min.js"></script>
  </body>
</html>